\section{Introduction}
\label{sec:introduction}

Serverless computing is an emerging cloud paradigm built upon the backbone of the Data Center Network (DCN). It abstracts away server management, allowing developers to focus solely on application logic~\cite{CACM19:Rise, TR19:Berkeley, CACM21:Serverless}. In this model, developers deploy programs as cloud functions, and the platform automatically manages the underlying infrastructure, including resource provisioning, auto-scaling and traffic routing. The simplicity and efficiency of serverless computing have led to widespread adoption in domains including web applications~\cite{Doc:AzureWebApp}, data processing pipelines~\cite{pu2019shuffling, OSDI18:Pocket}, and machine learning workloads~\cite{SC20:Batch, OSDI21:Dorylus}. For cloud providers, however, transforming serverless computing into a sustainable business model necessitates efficient resource utilization across a multi-tenant server fleet.

To achieve high resource utilization, prevailing approaches typically optimize for proxy objectives, such as balancing load~\cite{SoCC22:Hermod, Doc:Knative, Doc:OpenFaaS} or maximizing the reuse of initialized execution environments (often termed warm instances)~\cite{FaaSCache, ASPLOS24:RainbowCake, RoyPT22, ShahradFGCBCLTR20, SoCC22:Hermod, Doc:OpenWhisk}.
However, by largely ignoring the diverse resource footprints (\eg CPU-to-memory ratios) across different functions, these strategies can make sub-optimal routing decisions. If a router sends many memory-intensive functions to a single node, it risks exhausting available memory while letting CPUs remain underutilized. This fragmentation creates stranded resources, preventing the node from processing new requests and thereby lowering overall cluster utilization.

Furthermore, existing resource allocation strategies predominantly operate at the function level, where a static resource capacity is allocated to each function instance~\cite{Doc:AWS_Lambda_Mem, Doc:Azure_Functions_Hosting, Doc:GCP_CloudRun_Mem, FaaSCache, JinZXZHLJ23, zhang2024jolteon, ZhangTKCS21, ORION_MahgoubYSECB22, AQUATOPE_ZhouZD23}. While prior work recognizes that resource demands can vary across requests~\cite{CherryPick, Selecta, OPTIMUSCLOUD, ORION_MahgoubYSECB22, aws_lambda_power_tuning}, its solutions often rely on predicting an aggregate demand, selecting a single optimal static configuration~\cite{CherryPick, Selecta, OPTIMUSCLOUD}, or applying coarse-grained bin-packing~\cite{ORION_MahgoubYSECB22}. Consequently, these strategies remain fundamentally static and cannot efficiently adapt to fine-grained, input-dependent workload variations at the request level. This one-size-fits-all approach inevitably leads to inappropriate resource allocation, as it either over-provisions resources for light requests or under-provisions for heavy ones.

We confirm these inefficiencies by conducting a comprehensive analysis over a wide range of real-world serverless applications (\S\ref{sec:background:demand-analysis}). Our analysis shows that: i) The CPU-to-memory ratio (core/GB) varies significantly across different functions, ranging from 0.09 to 10.10, with a coefficient of variation (CV) of 62.43\%. This high inter-function heterogeneity leads to significant resource stranding under existing resource allocation strategies that ignore such diversity. ii) At the request level, 54\% of functions exhibit substantial variation in resource usage under different inputs, with a CV exceeding 50\%. This degree of variability far exceeds what static allocation can efficiently accommodate, forcing over-provisioning to avoid performance degradation and resulting in low resource utilization.

To overcome these inefficiencies, we present \sysname, a serverless computing system that handles the heterogeneous resource demands of functions and requests with a two-pronged solution. At the inter-node level, we introduce complementary swarming, a function request routing strategy that mitigates resource stranding by co-locating functions with opposing resource requirements while preserving locality. At the intra-node level, we design a monotonic scale-up request scheduling policy that reorders the request queue for a function to enforce monotonic resource adjustment which avoids the high overhead of instance restarts. Together, these techniques allow \sysname to significantly improve cluster utilization.

First, to address cluster-wide fragmentation, the complementary swarming routing strategy classifies functions based on their distinct resource profiles (\eg CPU-intensive vs. memory-intensive). It employs a multidimensional packing algorithm that treats resources as distinct dimensions, intentionally co-locating functions with opposing resource needs on the same physical node. This ensures that a node's full capacity is utilized across all dimensions simultaneously, preventing one resource from becoming a bottleneck while another sits idle.

Second, to address the overhead of handling input-dependent variance, \sysname utilizes Kubernetes' in-place resizing capability~\cite{Doc:K8s_Pod_Resize}, which effectively facilitates unidirectional scaling; while capacity can be seamlessly increased, reducing resources still requires a costly container restart. To minimize container restarts, we introduce monotonic scale-up request scheduling, which organizes function requests into queues sorted by monotonically increasing resource demands. A warm instance processes requests sequentially from a single queue, ensuring that the container only needs to scale up\textemdash never down\textemdash during a batch. This avoids the latency of cold starts and the overhead of frequent memory reclamation, effectively amortizing the restart cost across a batch of requests.

The key contributions of this paper are as follows:
\begin{itemize}
    \item We conduct a comprehensive characterization of real-world serverless workloads to determine and quantify the resource inefficiency caused by the heterogeneous resource demands of functions and requests.
    \looseness=-1
    \item We design \sysname, a serverless computing system that co-locates functions with complementary resource requirements and dynamically adjusts resources at the request level with complementary swarming for request routing and monotonic scale-up for request scheduling.
    \item We implement and evaluate \sysname on real-world workloads, demonstrating 41.41\%--67.01\% utilization improvement across multiple resources, and up to 2.03$\times$ overall throughput compared to state-of-the-art systems.
\end{itemize}
