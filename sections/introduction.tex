\section{Introduction}
\label{sec:introduction}

Serverless computing is an emerging cloud paradigm that abstracts away server management, allowing developers to focus solely on application logic~\cite{CACM19:Rise, TR19:Berkeley, CACM21:Serverless}. In this model, developers deploy programs as cloud functions, and the platform automatically manages the underlying infrastructure, including resource provisioning and auto-scaling. The simplicity and efficiency of serverless computing have led to widespread adoption in domains including web applications~\cite{Doc:AzureWebApp}, data processing pipelines~\cite{NSDI19:Shuffling, OSDI18:Pocket}, and machine learning workloads~\cite{SC20:Batch, OSDI21:Dorylus}. For cloud providers, however, realizing the potential of serverless demands extreme resource efficiency across multi-tenant server fleets.

The prevailing approaches to efficient resource management typically pursue specific objectives like balancing load~\cite{SoCC22:Hermod, Doc:Knative, Doc:OpenFaaS} or maximizing the reuse of warm instances~\cite{FaaSCache, ASPLOS24:RainbowCake, ASPLOS22:IceBreaker, ATC20:SitW, SoCC22:Hermod, Doc:OpenWhisk}. \yunshan{However, by largely ignoring the diverse resource footprints (e.g., CPU-to-memory ratios) across different functions, these schedulers can make sub-optimal packing decisions.} If a scheduler co-locates multiple memory-heavy functions on a node, it risks exhausting available RAM while CPU cycles remain underutilized. This fragmentation creates stranded resources, preventing the node from accepting new tasks and thereby lowering overall cluster utilization.

Furthermore, existing resource allocation strategies predominantly operate at the function level, where a static resource capacity is allocated to each function instance~\cite{Doc:AWS_Lambda_Mem, Doc:Azure_Functions_Hosting, Doc:GCP_CloudRun_Mem, FaaSCache, JinZXZHLJ23, zhang2024jolteon, ZhangTKCS21, ORION_MahgoubYSECB22,AQUATOPE_ZhouZD23}. \yunshan{While prior work recognizes that resource demands can vary across invocations~\cite{xxx}, its solutions often rely on predicting an aggregate demand, selecting a single “optimal” static configuration~\cite{xxx}, or applying coarse-grained bin-packing~\cite{xxx}. Consequently, these strategies remain fundamentally static and cannot efficiently adapt to fine-grained, input-dependent workload variations at the request level. This one-size-fits-all approach leads to significant resource inefficiency, as it either over-provisions resources for light requests or under-provisions for heavy ones.}

We confirm these inefficiencies by conducting a comprehensive analysis over a wide range of real-world serverless applications (\S\ref{sec:background:demand-analysis}). Our analysis shows that: \yunshan{i) The CPU-to-memory ratios (core/GB) of different functions vary widely, ranging from 0.09 to 9.20, with a coefficient of variation (CV) of 62.43\%. This high inter-function heterogeneity leads to significant resource stranding under existing placement strategies that ignore such diversity. ii) At the request level, 54\% of functions exhibit substantial variation in resource usage, with a CV exceeding 50\%. This degree of variability far exceeds what a static allocation can efficiently accommodate, forcing systematic over-provisioning to avoid performance degradation and resulting in low average utilization.} 

To overcome these inefficiencies, we present \sysname, a serverless computing system that handles heterogeneity of functions and requests with a two-pronged solution. At the inter-server level, we introduce complementary swarming, a function placement strategy that mitigates resource stranding by co-locating functions with opposing resource requirements while preserving locality. At the intra-server level, we design a monotonic scale-up request scheduling policy that reorders the request queue for a function to enforce monotonic resource adjustment which avoids the high overhead of instance restarts. Together, these techniques allow \sysname to significantly improve cluster utilization.

First, to address cluster-wide fragmentation, the complementary swarming placement strategy classifies functions based on their distinct resource profiles (e.g., CPU-intensive vs. memory-intensive). It employs a multidimensional packing algorithm that treats CPU and memory as distinct dimensions, intentionally co-locating functions with opposing resource needs on the same physical node. This ensures that a node's full capacity is utilized across all dimensions simultaneously, preventing one resource from becoming a bottleneck while another sits idle.

Second, to address the overhead of handling input-dependent variance, \sysname leverages recent Kubernetes techniques that allow the dynamic, in-place adjustment of a container's resource capacity~\cite{Doc:K8s_Pod_Resize}. We introduce monotonic scale-up scheduling, which organizes incoming requests into queues sorted by monotonically increasing resource demands. A warm instance processes requests sequentially from a single queue, ensuring that the container only needs to scale up \textemdash never down \textemdash during a batch. This avoids the latency of cold starts and the overhead of frequent memory reclamation, effectively amortizing the startup cost across a long chain of requests.

The key contributions of this paper are as follows:
\begin{itemize}
    \item We conduct a comprehensive characterization of real-world serverless workloads to determine and quantify the resource inefficiency caused by the heterogeneity of functions and requests.
    \item We design \sysname, a serverless computing system that co-locates functions with complementary resource requirements and dynamically adjust resources at the request level with complementary swarming for placement and monotonic scale-up for scheduling.
    \item We implement and evaluate \sysname on real-world workloads, demonstrating up to 55.53\% higher CPU utilization, 67.01\% higher memory utilization, and 103.71\% greater cluster throughput compared to state-of-the-art serverless~systems.
\end{itemize}
