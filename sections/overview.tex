% \section{Algorithm Design}

\section{\sysname Overview}
\label{sec:overview}

{
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/overview.pdf}
  \vspace{-15pt}
  \caption{\sysname overview. \junyi{put multiple users in the figure}}
  \vspace{-15pt}
  \label{fig:overview}
\end{figure}
}

Figure~\ref{fig:overview} illustrates the overall architecture of \sysname. The scheduling framework operates at two levels: inter-node request dispatching and intra-node request scheduling. \junyi{should probably unify task/request, server/node to avoid confusing people} At the cluster level, incoming user requests are first directed to a Request Dispatcher, which monitors traffic patterns. These traffic statistics, along with function metadata and performance metrics, are forwarded to the Instance Placement Manager. The manager periodically executes an algorithm to generate a placement plan, which is returned to the Request Dispatcher to guide request-to-node assignments.
Once assigned to a worker node, requests are entered into queues managed by the Worker Scheduler. This component maintains separate queues and priorities for different functions, while orchestrating instances within the node to handle request execution and in-place resource scaling. To enable this, \sysname introduces two key techniques: Complementary Swarming Instance Placement and Monotonic Scale-Up Task Scheduling.

\parabf{Complementary Swarming Instance Placement.} \junyi{based on the description, it sounds more like request placement instead of instance placement}
\sysname controls instance placement by establishing a mapping between a swarm of requests from the same function sharing similar resource demands range to worker nodes. This mapping dictates how requests are distributed across the cluster. Using a heuristic algorithm, \sysname ensures that the collective resource demand of the function mix on each worker node aligns with the node's capacity, thereby maximizing utilization. Simultaneously, the system minimizes the number of distinct functions per node to increase instance reuse. For high-demand functions requiring deployment across multiple servers, \sysname employs request-level load splitting to distribute the workload. By controlling the optimization step size, \sysname guarantees algorithm convergence and keeps computation time under 100 milliseconds.

\parabf{Monotonic Scale-up Request Scheduling.}
\sysname designs its worker-level scheduling algorithm around the restart-free in-place pod scaling feature of Kubernetes. The scheduler maintains dual request queues, both sorted in ascending order of resource demand. This ordering ensures that, within a time window, the resource requirements of tasks processed by an instance increase monotonically. Consequently, a pod can be scaled up incrementally to meet the growing demands of successive requests without over-provisioning, thereby increasing resource utilization without incurring restarts. To prevent starvation, \sysname determines queue priorities by holistically considering task waiting time, cold-start overhead, and resource requirements. By strategically reordering tasks to extend queue lengths under priority constraints, \sysname effectively boosts resource utilization and reduces the frequency of cold starts.