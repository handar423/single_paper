\section{\sysname Design}

\sysname employs a two-level hierarchical approach for scheduling function instances and requests, designed to maximize resource utilization.
At the inter-server level, a workload-splitting-based instance placement strategy operates to allocate resources and distribute function workloads across servers.
At the intra-server level, a vertical-scaling-based task scheduling algorithm configures container capacity based on request requirement, ensuring fine-grained resource provisioning.

\subsection{Workload-Splitting-based Instance Placement}

To address the significant variation in memory demands among requests for the same function, \sysname introduces a workload splitting mechanism.
This mechanism partitions a function's incoming requests into distinct categories based on their memory requirements and assigns each function instance to serve requests within a specific memory demand category.
This approach significantly reduces the memory footprint variation of containers for a given function, leading to more predictable resource consumption and simplified resource management.

Building upon this workload splitting, the instance placement strategy allocates resources by creating multiple instances for each function across different servers.
It then assigns a specific workload partition (i.e., a memory demand interval) to each instance, considering the available resources on its host server.
To maximize overall cluster utilization, the strategy colocates functions with complementary resource profiles (e.g., colocate CPU-intensive tasks with memory-intensive tasks), thereby minimizing resource stranding.

\parabf{Problem formulation.}
Given the historical workload characteristics and current queue length of all functions, \sysname predicts the aggregate resource requirements for each function.
\todo{More specific? How to predict resource requirements?} 
When distributing functions across physical servers, a function can be partitioned into multiple instances that reside on different servers and serve distinct workload partitions.
In this case, allocating more resources to a function allows it to launch more concurrent containers, thereby reducing request processing latency.

Formally, let the predicted aggregate CPU and memory requirements for function $i$ be $c_i$ and $m_i$, respectively.
The cluster consists of $N$ servers, where server $j$ has a capacity of $C_j$ CPU and $M_j$ memory.
The objective of our instance placement strategy is to determine an allocation of resources to function instances.
The resource allocation for function $i$ is represented by a set of tuples $\{(id_k, ac_k, am_k)\}$, where each tuple defines an instance $k$ residing on server $id_k$ and allocated $ac_k$ CPU and $am_k$ memory.
Concurrently, the strategy partitions the workload of function $i$.
If the function has $L$ instances, its request workload is partitioned into $L$ disjoint memory demand intervals: $[0, D_1), [D_1, D_2), \dots, [D_{L-1}, \infty)$. Requests whose memory demand falls into the $k$-th interval are routed to the $k$-th instance.

\parabf{Resource allocation.}
Our placement strategy commences by allocating resources to functions with the primary goal of minimizing resource stranding. This process consists of two stages.

In the first stage, starting from a state with no resource allocated, the algorithm iterates through each function to provision its initial resources.
The allocation maintains the same CPU-to-memory ratio for the resources provisioned on a server as the function's aggregate demand ratio ($c_i/m_i$).
The strategy prioritizes consolidating a function's instances onto a minimal number of servers to reduce the likelihood of cold starts. This greedy allocation process continues for all functions until all servers have run out of one type of resources (CPU or memory).

In the second stage, the algorithm refines the initial allocation to further improve resource utilization on each server.
In each iteration, it selects a function whose resource demands are not yet fully met and attempts to swap this function with one or more already-placed functions on a server.
The goal of a swap is to match the resource demand profile of the functions on a server with its available resource ratio, thereby reducing resource stranding.
For example, if a server has ample CPU but is memory-constrained, swapping a memory-intensive function for a CPU-intensive one can unlock the stranded CPU capacity.

Formally, for a candidate function with a CPU-to-memory ratio of $c/m$, and a server $j$ with currently allocated CPU and memory $C_j'$ and $M_j'$, we quantify the mismatch using a ratio gap:
\begin{equation}
    \text{Gap}_j=\left|\frac{c}{m}-\frac{C_j'}{M_j'}\right|.
\end{equation}

The algorithm selects the server $j$ with the maximum $\text{Gap}_j$.
It then identifies functions already on server $j$ to evict and replace with the candidate function.
To maximize the effectiveness of the swap, the eviction process prioritizes existing functions whose CPU-to-memory ratio $c_k/m_k$ has the largest deviation from the candidate function's ratio $c/m$.
This replacement process continues until the candidate function's demand is met or no further beneficial swaps can be made on the server.
The refinement stage iterates until no further improvements in server resource utilization can be achieved across the cluster.

\parabf{Workload splitting.}
Once resource allocation is complete, the strategy proceeds to the workload splitting phase for each function.
The workload is divided among a function's instances proportionally to their allocated memory, ensuring that instances with more resources are assigned a larger share of the total work.


Formally, for a function with $L$ instances, where instance $k$ is allocated $am_k$ memory, the target workload proportion for instance $k$, denoted as $W_k$, is:
\begin{equation}
    W_k=\frac{am_k}{\sum_{j=1}^L am_j}.
\end{equation}


As described in~\S\ref{sec:demand-analysis}, the memory requirements of a function's requests follow a known probability distribution $P(m)$.
We partition the entire memory demand range into $L$ intervals $I_1, \dots, I_L$ such that the expected memory consumption of requests in interval $I_k$ matches the target proportion $W_k$:
\begin{equation}
    \frac{\sum_{m\in I_k} P(m)\times m}{\sum_{m} P(m)\times m}\approx W_k.
\end{equation}

This ensures that the total workloads directed to an instance is proportional to its allocated memory.
Consequently, the $k$-th instance is configured to exclusively handle requests whose memory demands fall within the interval $I_k$.

\parabf{Runtime adjustment}.
As the predicted resource requirements of functions vary over time,
we adjust the function placement at runtime.
For functions that have decreased resource requirements, we shrink all its instances at the same proportion.
This may give chance for other functions to utilize the released resources for creating new instances.
In the case of increased resource requirements, we first withdraw all resources of the function, and then perform reallocation.
This reallocation can help resources of the function concentrate on several servers, mitigating potential cold starts.



\parabf{Runtime adjustment.}
Function resource requirements can fluctuate over time.
\sysname adapts to these changes through runtime adjustments to the placement.
When a function's predicted demand decreases, \sysname proportionally shrinks all of its instances, releasing resources back to the cluster.
These freed resources can then be utilized by other functions.
Conversely, if a function's demand increases, \sysname employs a more aggressive strategy: it retracts all resources currently allocated to the function and triggers a full reallocation for it.
This \emph{retract-and-reallocate} approach helps consolidate the function's newly required resources onto fewer servers, thereby mitigating cold starts that could arise from fragmented, small-scale additions.

To mitigate long-term resource fragmentation and adapt to major shifts in workload patterns, \sysname also performs a periodic, global reallocation. This involves releasing all resources from all functions and re-executing the entire two-stage placement strategy from scratch.
This global reset ensures that the cluster maintains high resource utilization and adapts effectively to evolving workloads.

\subsection{Vertical-Scaling-based Task Scheduling}

While the instance placement strategy operates at the cluster level, the task scheduler operates at the intra-server level, managing how requests are executed by the instances hosted on a single server.
The core principle of our task scheduler is to provision precisely the amount of memory required by each individual request, a technique we refer to as per-request vertical scaling.
To achieve this, the scheduler adjusts a container's memory capacity immediately before it executes a request to match that request's specific demand. This fine-grained resource management minimizes internal fragmentation within containers, thereby improving overall server resource utilization.

Notably, the overhead of this adjustment is asymmetric: increasing a container's memory capacity is a fast metadata operation with negligible overhead, whereas decreasing it can be slow, often requiring costly page reclaiming or even a full container restart.
To capitalize on the resources saved through vertical scaling, \sysname also dynamically adjusts the degree of parallelism (i.e., the number of active containers) for each instance on the server.
This autoscaling of containers is guided by workload pressure and resource availability, ensuring that freed resources are promptly reallocated to improve throughput.

\parabf{Problem formulation.}
The inputs to the intra-server task scheduler are the set of function instances assigned to the server by the placement strategy, along with their corresponding workload partitions.
The task scheduling algorithm runs independently on each server.
Its responsibilities include: (1) controlling the number of concurrent containers for each instance, and (2) determining the execution order of pending requests to minimize the overhead associated with memory reconfiguration.

Formally, consider a server with a total capacity of $C$ CPU and $M$ memory. It hosts $K$ function instances, and for each instance $i$, there is a queue of pending requests $q_i$.
The algorithm sorts these pending requests in a specific order.
When a container becomes available, it dequeues a request, and its memory capacity is vertically scaled to match the request's exact requirement before execution.
The CPU allocated to the container is set proportionally to its new memory capacity, maintaining the function's overall CPU-to-memory ratio.
Finally, the algorithm must dynamically determine the number of containers, $n_i$, for each instance $i$, subject to the server's resource constraints.



\parabf{Request scheduling.}
To mitigate the high cost of decreasing container memory, our scheduling policy for each instance utilizes a dual-queue structure: a \emph{running queue} and a \emph{backup queue}. Within both queues, requests are always sorted in ascending order of their memory demands.

An active container for the instance exclusively pulls requests from the head of the running queue. Before executing the request, the container's memory is adjusted to match the request's demand. Because the running queue is sorted by increasing memory, this adjustment is always an increase, which is a fast operation with negligible overhead.

When a new request arrives for the instance, the scheduler attempts to insert it into the running queue while maintaining the sorted order. If the new request's memory demand is smaller than that of any request currently in the running queue, it cannot be inserted there without violating the ascending-memory-only processing order. In this case, it is instead inserted into the backup queue (also maintaining sorted order).

Obviously, the running queue will ultimately be exhausted as the memory capacity of containers grows.
At this time, the scheduler performs a \emph{queue swap}: the backup queue becomes the new running queue, and the (now empty) running queue becomes the new backup queue.
Following a queue swap, containers processing requests from the new running queue will likely need to shrink its memory, which incur a container restart.
This design amortizes the high cost of memory reduction: a costly shrink operation occurs at most once per full cycle of the running queue, rather than on a per-request basis.


\parabf{Container autoscaling.}
As containers are vertically scaled to match request demands, the total memory consumed by an instance's containers fluctuates. To prevent Out-of-Memory (OOM) errors and fully utilize server capacity, \sysname dynamically scales the number of containers for each instance.

The scaling decision is driven by a priority score calculated for each instance. When resources become available, \sysname attempts to launch a new container for the instance with the highest priority.
Conversely, if an active container needs to scale up its memory but insufficient resources are free, \sysname preempts a container belonging to the instance with the lowest current priority to free up resources.

The priority of an instance is calculated based on the state of its pending requests. For an instance with $S$ active containers and a running queue $q=\{r_1, \dots, r_h\}$, where the corresponding memory demands are $\{m_1, \dots, m_h\}$ (sorted such that $m_1 \le \cdots \le m_h$), let $t$ be the waiting time of the head request, $r_1$. The priority for this instance is computed as:
\begin{equation}
    \text{Priority}=\frac{\alpha\cdot m_h + \beta\cdot t - \gamma\cdot \frac{m_h}{m_1}}{S}.
\end{equation}
\noindent where $\alpha, \beta, \gamma$ are configurable weighting parameters.

The priority calculation is composed of three main components:
\begin{itemize}
    \item The term $\alpha \cdot m_h$ prioritizes instances with high-memory requests pending. This is because these requests often have the longest execution times, and failing to schedule them promptly can lead to high tail latency. The value $m_h$ represents the peak memory requirement in the current queue cycle.
    \item The term $\beta \cdot t$ incorporates the waiting time of the head-of-queue request. This is a standard fairness metric, preventing starvation and ensuring that long-waiting requests are eventually served.
    \item The term $-\gamma \cdot \frac{m_h}{m_1}$ acts as a penalty for instances with high memory demand variance within their current running queue. A smaller ratio $m_h/m_1$ indicates greater memory stability, which is preferred as it leads to more predictable resource usage patterns.
\end{itemize}
The entire expression is divided by $S$, the current number of containers, to normalize the priority and favor scaling up instances that are currently underserved relative to their workload pressure.
