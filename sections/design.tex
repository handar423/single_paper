\section{\sysname Design}

\yunshan{\sysname employs a two-level hierarchical scheduling approach for function 
instances and requests: complementary swarming placement at the inter-server 
level and monotonic scale-up scheduling at the intra-server level.}

\subsection{Complementary Swarming Instance Placement}
\label{sec:design:placement}

The complementary swarming placement algorithm addresses the scheduling challenge 
posed by heterogeneous function resource demands, as quantified in 
\S\ref{sec:background:demand-analysis}. It pursues a dual objective: i) maximizing 
cluster-wide resource utilization by co-locating functions with complementary CPU 
and memory profiles on the same server, thereby mitigating resource stranding; 
and ii) enhancing instance reuse by deliberately narrowing the set of distinct 
functions served per node. To achieve this, the algorithm proceeds iteratively. 
In each round, it assigns the workload of a function to one or more worker nodes 
based on the function‘s resource profile and, when necessary, rolls back partial 
assignments of other already-placed functions to improve complementary packing. 
For functions requiring distribution across multiple nodes, the algorithm performs 
fine-grained load splitting according to the function resource demand distribution. 
The algorithm outcome is a stable task-to-node mapping that balances complementary 
packing with equitable load distribution.

\subsubsection{Modeling Node Capacity and Function Demand}
\label{subsec:modeling}

A precise formulation of the placement problem requires clear representations of 
both node resource capacity and function resource demand.

\textbf{Node Resource Capacity.} We model a worker node $N_j$’s available capacity 
as a two-dimensional resource vector $\mathbf{C}_j = \left(C_j^{\text{cpu}}, C_j^{\text{mem}}\right)$, 
representing its spare CPU cores and memory (in GB). Similarly, we denote by 
$\mathbf{C}_j^{\text{idle}} = \left(C_j^{idle,cpu}, C_j^{idle,mem}\right)$ the 
vector of idle (currently unallocated) resources on $N_j$ at any given moment. 
For simplicity and clarity of exposition, we focus on CPU and memory as the 
primary bottleneck resources. Our algorithm can be extended to incorporate other 
resources (\emph{e.g.}, network bandwidth) by increasing the dimensionality of 
the capacity vector. To quantify the efficiency of a placement, we define the 
\textbf{overall resource utilization} of a node \(N_j\) as the sum of its normalized 
CPU and memory utilization:
\[
U(N_j) = \frac{U_j^{\text{cpu}}}{C_{\text{total}}^{\text{cpu}}} + \frac{U_j^{\text{mem}}}{C_{\text{total}}^{\text{mem}}}
\]
where \(U_j^{\text{cpu}}\) and \(U_j^{\text{mem}}\) are the allocated resources 
on \(N_j\), and \(C_{\text{total}}^{\text{cpu}}\) and \(C_{\text{total}}^{\text{mem}}\) 
are the \textbf{total capacities of the entire cluster}. This definition inherently 
balances both resource dimensions. A placement or swapping operation is considered 
beneficial if it increases \(U(N_j)\). (The formulation can be extended with 
resource-specific weights if needed.)

\textbf{Function Resource Demand.} Modeling the resource demand of a function is 
more nuanced. We characterize it along three orthogonal dimensions:

\begin{enumerate}
    \item \textbf{Demand Ratio ($r_i$)}: The intrinsic CPU-to-memory ratio required 
    by a single execution of function $F_i$. The value of $r_i$ quantitatively 
    indicates a function’s resource affinity: a relatively small $r_i$ signifies 
    memory-intensive behavior, whereas a relatively large $r_i$ corresponds to 
    compute-intensive behavior. To reduce problem complexity, we assume this 
    ratio remains invariant across invocations of the same function.

    \item \textbf{Demand Volume ($\mathbf{V}_i$)}: The aggregate resource demand 
    vector for function $F_i$ within the scheduling window, denoted as 
    $\mathbf{V}_i = \left(V_i^{\text{cpu}}, V_i^{\text{mem}}\right)$. Here, $V_i^{\text{cpu}}$ and 
    $V_i^{\text{mem}}$ represent the predicted total CPU (\emph{e.g.}, in core-seconds) 
    and memory (\emph{e.g.}, in GB-seconds) required to process all expected 
    invocations of $F_i$ in that window, respectively. While the accurate 
    prediction of $V_i$ is orthogonal to our placement algorithm (and its accuracy 
    naturally affects overall performance), our design accepts such a predicted 
    vector as input.

    \item \textbf{Demand Distribution ($D_i$)}: The statistical distribution of 
    per-invocation resource requirements, derived from historical traces. As shown 
    in Fig.~\ref{fig:usage_patterns}, these distributions exhibit diverse shapes 
    (\emph{e.g.}, quasi-normal, long-tailed). $D_i$ is crucial for fine-grained 
    load splitting when a function must be spread across multiple nodes.
\end{enumerate}
Together, the tuple $\left<r_i, \mathbf{V}_i, D_i\right>$ forms a comprehensive 
demand profile for each function $F_i$, enabling our algorithm to reason about 
complementary placement and proportional workload distribution simultaneously.

{
% \setlength{\belowdisplayskip}{-140pt}
\begin{algorithm}[t]
    \caption{\mbox{Complementary swarming placement.}}
    \noindent \textbf{Data:}
    \begin{algorithmic}[1]
    \Statex \begin{itemize}
        \setlength{\itemindent}{.0em} % 控制缩进
        \item Pending assignment vector \(\mathbf{R} \in [0,1]^n\).
        \item Assignment matrix \(\mathbf{L} \in [0,1]^{m \times n}\).
    \end{itemize}
    \Require $ $
    \Statex \begin{itemize}
        \setlength{\itemindent}{.0em} % 控制缩进
        \item Node set \(\mathcal{N} = \{N_1, \dots, N_m\}\).
        \item Function set \(\mathcal{F} = \{F_1, \dots, F_n\}\).
    \end{itemize}
    \Ensure Optimized placement $\mathcal{P}: \mathcal{F} \times [0,1] \to \mathcal{N}$.
    
    \State \textbf{// Step 1: Initialize state \& normalize function demands}
    \State \(\mathbf{L} \gets \mathbf{0}_{m \times n}\), \(\mathbf{R} \gets \mathbf{1}_n\)
    \For{$F_i \in \mathcal{F}$}
        \State $\mathbf{V}_i \gets \mathbf{V}_i \times \frac{\sum_{j=1}^{m} \mathbf{C}_j}{\sum_{i=1}^{n} \mathbf{V}_i}$ \Comment{Scale demand proportionally}
    \EndFor
    \State
    
    \State \textbf{// Step 2: Main optimization loop}
    \While{$\exists \; F_i \text{ s.t. } R[i] > 0$} \Comment{Placement incompletement}
        \State $F_a \gets \text{RandomChoice}(\{F_i \mid R[i] > 0\})$ 
        \For{$N_j \in \mathcal{N}$}
            \State \textbf{// Assignment via free capacity}
            \State $\alpha_j^{\text{free}} \gets min(\frac{C_j^{\text{idle,cpu}}}{V_i^{\text{cpu}}}, \frac{C_j^{\text{idle,mem}}}{V_i^{\text{mem}}})$
            \State \textbf{// Assignment via beneficial swaps}
            \State $(\alpha_j^{\text{swap}},\; \mathcal{E}_j) \gets \text{SwapCapacity}(F_a, N_j, \mathbf{L}, \mathbf{C}_j^{idle})$
            \Comment{$\mathcal{E}_j$: list of $(F_k, \beta_k)$ evicted}
            \State $\alpha_j^{\text{max}} \gets min(\alpha_j^{\text{free}} + \alpha_j^{\text{swap}}, R[a])$
            \State $\Delta U_j \gets \text{UtilGain}(F_a, \alpha_j^{\text{swap}}, \mathcal{E}_j, \mathbf{C}_j, \mathbf{C}_{\text{total}})$
        \EndFor

        \State \textbf{// Decision \& Assignment (ignoring edge cases)}
        \For{$N_j \in \mathcal{N}$, \textbf{descending} order of $\Delta U$}
            \If{$R[a] \leq 0$} \textbf{break} \EndIf
            \State $L[j, a] \gets L[j, a] + \alpha_j^{\text{max}}$, $R[a] \gets R[a] - \alpha_j^{\text{max}}$
            \For{$(F_k, \beta_k)$ in $\mathcal{E}_j$}
                \State $L[j, k] \gets L[j, k] - \beta_k$, $R[k] \gets R[k] + \beta_k$
            \EndFor
        \EndFor
    \EndWhile
    \State

    \State \textbf{// Step 3: Generate $\mathcal{P}$ from $L$}
    \For{$F_i \in \mathcal{F}$}
        \State $p_{\text{start}} \gets 0.0$ \Comment To store workload percentile range
        \For{$N_j \in \mathcal{N}$, where $L[j,i] > 0$, in arbitrary order}
            \State // We use half‑open intervals $[p_s, p_e)$, except the last interval which is closed at $1$.
            \State $\mathcal{P}(F_i, [p_{\text{start}}, p_{\text{start}} + L[j,i])) \gets N_j$ 
            \State $p_{\text{start}} \gets p_{\text{start}} + L[j,i]$
        \EndFor
    \EndFor
    \State \Return $\mathcal{P}$


\end{algorithmic}
\label{alg:placement}
\end{algorithm}
}

\subsubsection{Complementary Swarming Placement Algorithm}
\label{subsec:placement-algorithm}

Algorithm~\ref{alg:placement} presents the complementary swarming placement 
algorithm. It takes as input the set of nodes $\mathcal{N}$ with their capacity 
vectors $\mathbf{C}_j$, and the set of functions $\mathcal{F}$ with their demand 
profiles $\langle r_i, \mathbf{V}_i, D_i \rangle$. The algorithm outputs a placement 
mapping $\mathcal{P}: \mathcal{F} \times [0,1] \to \mathcal{N}$. This mapping 
determines task dispatch: when a function's workload is split across multiple 
nodes, $\mathcal{P}$ assigns each percentile of the function's resource‑demand 
distribution to a specific node. This percentile‑based splitting is illustrated 
in Fig.~\ref{fig:placement_p}.

\textbf{Step 1: Initialization \& Demand Normalization.}
The algorithm maintains two key data structures initialized in this step: i) a 
pending assignment vector $\mathbf{R}$, where $R[i]$ denotes the fraction of 
$F_i$ demand yet to be placed, initialized to $1$ for all functions; and ii) an 
assignment matrix $\mathbf{L}$, where $L[j,i]$ records the fraction of $F_i$ 
demand currently assigned to node $N_j$, initialized to all zeros (line~2). 
Furthermore, we scales the demand vector $\mathbf{V}_i$ of each function 
proportionally so that the aggregate demand matches the total cluster capacity 
($\sum_i \mathbf{V}_i = \sum_j \mathbf{C}_j$). This normalization ensures the 
subsequent placement operates under a balanced supply-demand premise (Line~3--4). 

\textbf{Step 2: Iterative Optimization Loop.}
The core of the algorithm is an iterative loop that runs until all function demands 
are placed ($R[i]=0, \forall i$, Line~7). In each iteration, it randomly selects 
a function $F_a$ with pending demand ($R[a] > 0$, Line~8). For each node $N_j$, 
the algorithm evaluates two ways to accommodate $F_a$: i) using the idle node 
resources, which yields a capacity $\alpha_j^{\text{free}}$ computed as the 
minimum ratio of idle CPU/memory to $F_a$ demand (Line~11); and ii) through 
beneficial swaps, where the subroutine $\text{SwapCapacity}$ attempts to evict 
already-placed functions in order to host as much of $F_a$ demand as possible, 
provided that doing so improves the node utilization $U_j$ (Line~13, detailed 
in Algorithm~\ref{alg:swap-capacity}). $\text{SwapCapacity}$ returns both the 
maximum $F_a$ demand fraction $\alpha_j^{\text{swap}}$ that can be hosted by 
function swaping, and a list $\mathcal{E}_j$ of evicted functions, each with the 
evicted load fraction $\beta$. The total accommodation capacity for $F_a$ on 
$N_j$ is then $\alpha_j^{\text{max}} = \min(\alpha_j^{\text{free}}+\alpha_j^{\text{swap}}, R[a])$ 
(Line~14).

After evaluating all nodes, the algorithm sorts them in descending order of the 
utilization gain $\Delta U_j$ (Line~17) and assigns $F_a$ pending demand greedily. 
For each node in this order, it assigns up to $\alpha_j^{\text{max}}$ of $F_a$ demand, 
updates $\mathbf{L}$ and $\mathbf{R}$ (Line~19), and simultaneously processes 
the evictions in $\mathcal{E}_j$: for each function $F_k$ in $\mathcal{E}_j$, 
the algorithm reducing the evicted fraction $\beta_k$ in $\mathbf{L}[j, k]$ and 
adds $\beta_k$ back to $F_k$ pending demand $\mathbf{R}[k]$ (Line~21). This process 
continues until $F_a$ demand is fully placed ($R[a]=0$). Because the total 
function demand was scaled to match the total cluster capacity in Step 1, it is 
guaranteed that $F_a$ demand can eventually be fully accommodated, and the loop 
will not terminate with $R[a] > 0$. To ensure the loop convergence speed, the 
algorithm skips nodes whose utilization gain $\Delta U_j$ is below a predefined 
empirical threshold (\emph{e.g.}, 1\%).

{
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/design/placement_p.pdf}
  \vspace{-10pt}
  \caption{Generate $\mathcal{P}$ from $L$,.}
  \vspace{-10pt}
  \label{fig:placement_p}
\end{figure}
}

\textbf{Step 3: Fine-Grained Placement Generation.}
After the assignment matrix $\mathbf{L}$ is determined, the algorithm converts 
the proportional assignments into a precise placement mapping $\mathcal{P}$ 
(Line~24--29). This process is illustrated in Fig.~\ref{fig:placement_p}. 
For each function $F_i$, the algorithm sequentially allocates contiguous percentile 
intervals in $[0, 1]$ to each node $N_j$ that hosts $F_i$ (left side of the figure). 
The length of the interval assigned to $N_j$ is exactly $L[j,i]$. This completes 
the construction of $\mathcal{P}$, which is the algorithm’s final output. In 
practice, to avoid resource fragmentation that could arise if certain nodes only 
receive large tasks, the intervals are assigned to nodes in a randomized order 
rather than a fixed sequence; the figure omits this step for simplicity.

\parabf{Task Dispatching.} At runtime, the interval $[0, 1]$ is concretely mapped 
to the resource‑demand percentiles of $F_i$’s invocations (right side of 
Fig.~\ref{fig:placement_p}). Specifically, using $F_i$’s historical demand 
distribution $D_i$, we pre‑compute the resource‑demand boundaries that correspond 
to each percentile sub‑interval. When a new invocation arrives, its actual resource 
demand (derived from input parameters and data size) is located on $D_i$ to obtain 
its percentile $p$. The invocation is then dispatched to the node $N_j$ for which 
$\mathcal{P}(F_i, p) = N_j$. For example, the newly arrived invocation marked in 
the figure, whose demand percentile falls inside $[0.8, 1]$, would be dispatched 
to Node C. Our experiments obtain accurate demand estimates from the benchmark’s 
known input characteristics; the algorithm can be coupled with more sophisticated 
prediction methods for further performance gains.

\begin{algorithm}
\caption{SwapCapacity: Compute beneficial swap capacity}
\label{alg:swap-capacity}
\begin{algorithmic}[1]
\Require $ $
\Statex \begin{itemize}
    \setlength{\itemindent}{.0em} % 控制缩进
    \item Node $N_j$ with idle resource vector $\mathbf{C}_j^{\text{idle}}$
    \item Target function $F_a$
    \item Allocation matrix $\mathbf{L}$
\end{itemize}
\Ensure Swap capacity $\alpha^{\text{swap}}$, eviction list $\mathcal{E} = [ (F_k, \beta_k) ]$
\State $\alpha^{\text{swap}} \gets 0$, $\mathcal{E} \gets [\,]$, $\{F_k\} \gets \{F_k | L[j, k] > 0\}$
\If{$C_j^{\text{idle,cpu}} > 0$} \Comment Determine dominant idle resource
    \State // Replace more memory‑intensive first
    \State sort $\{F_k\}$ by $r_k$ \textbf{ascending} ; $\text{sign} \gets +1$
\Else \Comment{ $C_j^{\text{idle,mem}} > 0$ }
    \State // Replace more compute‑intensive first
    \State sort $\{F_k\}$ by $r_k$ \textbf{descending}; $\text{sign} \gets -1$
\EndIf
\For{each $F_k$ in the sorted order}
    \If{$\text{sign} \cdot (r_a - r_k) \leq 0$} 
        \State \textbf{continue} \Comment{Swap would not improve $U(N_j)$}
    \EndIf
    \State Compute max $\beta_k$ s.t. evicting $\beta_k$ of $F_k$ respects $\mathbf{C}_j^{\text{idle}}$
    \State $\beta_k \gets \min(\beta_k,\; L[j, k])$
    \State $\gamma_k \gets \beta_k \cdot (\mathbf{V}_k \oslash \mathbf{V}_a)$ 
        \Comment{$\gamma_k = \beta_k \cdot \min\left(\frac{V_k^{\text{cpu}}}{V_a^{\text{cpu}}}, \frac{V_k^{\text{mem}}}{V_a^{\text{mem}}}\right)$}
    \If{$\alpha^{\text{swap}} + \gamma_k > R[a]$}
        \State $\gamma_k \gets R[a] - \alpha^{\text{swap}}$; adjust $\beta_k$ proportionally
    \EndIf
    \State $\alpha^{\text{swap}} \gets \alpha^{\text{swap}} + \gamma_k$
    \State $\mathcal{E} \gets \mathcal{E} \;\cup\; \{ (F_k, \beta_k) \}$
    \If{$\alpha^{\text{swap}} \geq R[a]$} \textbf{break} \EndIf
\EndFor
\State \Return $(\alpha^{\text{swap}},\; \mathcal{E})$
\end{algorithmic}
\end{algorithm}

\parabf{Assignment via beneficial swaps.}
The SwapCapacity subroutine (Algorithm~\ref{alg:swap-capacity}) determines, for 
a given node $N_j$ and a target function $F_a$, the maximum additional fraction 
$\alpha_j^{\text{swap}}$ of $F_a$ demand that can be hosted by beneficially evicting 
already-placed functions. A swap is considered beneficial only if it increases 
the overall node utilization $U(N_j)$. This requirement dictates that swaps must 
consume the node’s dominant idle resource: if CPU is idle, $F_a$ must replace 
more memory-intensive functions (with smaller demand ratio $r_k$); if memory is 
idle, it must replace more compute-intensive ones (with larger $r_k$). The subroutine 
takes as input the node idle resource vector $\mathbf{C}_j^{\text{idle}}$, the 
target function $F_a$, and the current assignment matrix $\mathbf{L}$; it returns 
the admissible swap capacity $\alpha^{\text{swap}}$ together with a list $\mathcal{E}$ 
of evicted functions, each annotated with the evicted load fraction $\beta_k$.

The subroutine proceeds in four logical steps. First, it identifies the dominant 
idle resource—CPU or memory—since after using the node’s free capacity (line~11 
of Algorithm~\ref{alg:placement}), only one resource dimension may remain idle (lines 2--7). 
This determines the order in which existing functions are examined: if CPU is 
idle, functions are sorted by their resource-demand ratio $r_k$ in ascending 
order (\emph{i.e.}, more memory‑intensive functions first); if memory is idle, 
they are sorted in descending order (more compute‑intensive first). This ordering 
ensures that the most beneficial candidates are tried first, because replacing 
a function with a very complementary resource profile allows $F_a$ to consume 
more of the idle resource.

Second, for each candidate function $F_k$ in this sorted list, the algorithm 
performs a benefit test (line~9). The condition $\operatorname{sign}\cdot(r_a-r_k) > 0$ 
guarantees that swapping a fraction of $F_k$ for $F_a$ will actually increase 
$U(N_j)$. Intuitively, if CPU is idle ($\operatorname{sign}=+1$), we must have 
$r_a > r_k$, meaning $F_a$ is more compute‑intensive than $F_k$ and will better 
utilize the spare CPU; the converse holds when memory is idle. Candidates that 
fail this test are skipped.

Third, for a candidate that passes the test, the algorithm computes how much of $F_k$ can be evicted ($\beta_k$) without exceeding the node’s idle resources, and limits $\beta_k$ by the amount currently assigned to $N_j$ ($L[j,k]$) (lines~11--12). It then translates this evicted capacity into an equivalent amount of $F_a$ that can be hosted using the freed resources. The translation uses the formula $\gamma_k = \beta_k \cdot \min\left(\frac{V_k^{\text{cpu}}}{V_a^{\text{cpu}}}, \frac{V_k^{\text{mem}}}{V_a^{\text{mem}}}\right)$(line~13), which ensures that neither CPU nor memory exceeds the vacated capacity—the $\min$ operation enforces the tighter of the two resource constraints.

Finally, the algorithm accumulates the admissible fractions $\gamma_k$ until 
either the pending demand $R[a]$ of $F_a$ is exhausted or no further beneficial 
swaps can be found (lines~14--17). The accumulated total $\alpha^{\text{swap}}$ 
and the corresponding eviction list $\mathcal{E}$ are returned to the main placement loop.

\parabf{Low Overhead via Partial Updates.} To keep the scheduling overhead low, 
the placement algorithm employs an incremental (partial) update mechanism. Rather 
than recomputing the full placement from scratch for every scheduling window, the 
system periodically (every minute in our implementation) monitors the latest 
demand profiles $\langle r_i, \mathbf{V}_i, D_i \rangle$ of all functions. For 
a function whose aggregate demand $\mathbf{V}_i$ has not increased and demand 
distribution $D_i$ remains statistically stable, the algorithm preserves its 
current placement. Only functions with significantly changed demand (increased 
$\mathbf{V}_i$ or altered $D_i$) are fed into the complementary swarming algorithm 
to update their assignments. This partial update drastically reduces the number 
of functions that need to be re‑placed in each iteration, which in turn minimizes 
instance migration, thereby reducing scheduling churn and preserving warm sandbox 
states. Furthermore, to prevent the accumulation of sub‑optimality that may arise 
from multiple incremental updates, the system performs a global re‑placement 
every 30 minutes, which recomputes the assignment for all functions from a clean 
state, ensuring long‑term placement quality.

\subsection{Monotonic Scale-up Task Scheduling}
\label{sec:design:scheduling}

{
\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/design/scheduling.pdf}
  \vspace{-10pt}
  \caption{Monotonic acale-up task scheduling workflow.}
  \vspace{-10pt}
  \label{fig:scheduler-arch}
\end{figure}
}

\label{subsec:scheduler-structure}

The monotonic scale‑up scheduler runs on each worker node. Its primary objective 
is to handle the intra‑function demand variability identified in 
\S\ref{sec:background:demand-analysis} by leveraging in‑place vertical scaling, 
thereby avoiding costly cold starts while maintaining high resource efficiency.

\subsubsection{Scheduler Architecture}

As illustrated in Fig.~\ref{fig:scheduler-arch}, the core scheduling structure 
is organized per function. For each function $F_i$, the scheduler maintains two 
monotonic task queues: a current queue and a backup queue. Tasks in both queues 
are ordered by their predicted resource demand in increasing order. This monotonic 
ordering is the key to enabling efficient in‑place scaling: when a sandbox processes 
tasks sequentially from a single queue, its resource allocation can be increased 
incrementally without~restarts.

\parabf{Task‑insertion rule.} When a new invocation of $F_i$ arrives, its predicted 
resource demand is compared with the demand of the head (smallest) task in the 
current queue. If the new demand is smaller, the invocation is placed into the 
backup queue; otherwise, it is inserted into the current queue (preserving the 
monotonic order). The purpose of this rule is not merely to keep each queue sorted, 
but to guarantee that a sandbox, while processing tasks from a single current 
queue, encounters a resource demand sequence that is monotonically non‑decreasing. 
This property is essential: if a sandbox were forced to run a larger task before 
a smaller one, it would either have to start at the higher capacity (wasting 
resources) or be restarted after a down‑scale (incurring cold‑start overhead). 
The backup queue temporarily holds smaller tasks until the current queue is 
fully processed, after which the roles of the two queues may swap.

\parabf{Sandbox execution and priority‑based selection.} A function may have from 
zero to multiple concurrent warm sandboxes, each dedicated to fetching tasks only 
from that function’s current queue. The scheduler assigns a dynamic priority to 
each function (detailed in \S\ref{subsec:priority-design}). At every scheduling 
point—triggered by a new task arrival or a task completion—the scheduler selects 
the function with the highest priority and attempts to dispatch the head task of 
its current queue.

\parabf{Resource‑assignment policy.} To execute the selected task, the scheduler 
tries the following options in order, aiming to minimize overhead:

\begin{enumerate}[label=\roman*)]
    \item \textbf{Reuse} an existing sandbox of the same function whose current 
    resource allocation already meets or slightly exceeds the task’s demand, 
    thereby avoiding any scaling~operation.
    
    \item \textbf{In‑place scaling} of a warm sandbox via restart‑free vertical 
    scaling (Kubernetes Pod resize API) to accommodate the task, thus preserving 
    warm state.

    \item \textbf{Launch a new sandbox} (cold start) if no suitable warm sandbox 
    is available.
\end{enumerate}

If the node lacks sufficient idle resources for options ii) or iii), the scheduler 
evaluates eviction. It selects candidate sandboxes according to a per‑sandbox 
eviction priority computed as:
\[
\text{Prio}_{\text{sandbox}} = \frac{\text{Prio}_{\text{function}}}{N_{\text{sandboxes}}},
\]
where \(N_{\text{sandboxes}}\) is the number of active sandboxes belonging to 
the same function. Among sandboxes of the same function, those with larger resource 
footprints are evicted first. This scheme encourages load spreading (by penalizing 
functions that already hold many sandboxes) and promotes fair resource sharing. 
Eviction is performed only if it can actually release enough resources to execute 
the pending task; otherwise, the scheduler leaves the system unchanged and waits 
for the next scheduling event.

\subsubsection{Priority Design}
\label{subsec:priority-design}

The scheduler assigns a dynamic priority to each function $F_i$ to balance three 
objectives: minimizing cold‑start overhead, preventing task starvation, and 
maximizing the utility of in‑place scaling. The priority is computed as a 
weighted sum of three factors derived from the state of $F_i$’s~queues:
\[
\text{Priority}(F_i) = \omega_1 \cdot \text{factor}_1 \;+\; \omega_2 \cdot \text{factor}_2 \;+\; \omega_3 \cdot \text{factor}_3,
\]

where the weights $\omega_1, \omega_2 > 0$ and $\omega_3 < 0$ are tunable 
parameters (set to $0.3$, $1.1$ and $-0.8$ respectively in our implementation). 
The three factors are defined as follows:

\begin{itemize}
    \item \textbf{Cold‑start cost factor} ($\text{factor}_1$).  
    Let $t_{\text{last}}$ be the task with the largest predicted resource demand 
    in $F_i$’s current queue, and let $C_{\text{cold}}(F_i)$ denote the estimated 
    cold‑start cost of $F_i$ (\emph{i.e.}, the time overhead of launching a new 
    sandbox). Then
    \[
    \text{factor}_1 = \max\!\left(\frac{t_{\text{last}}^{\text{mem}}}{C_j^{\text{mem}}},\; \frac{t_{\text{last}}^{\text{cpu}}}{C_j^{\text{cpu}}}\right) \cdot C_{\text{cold}}(F_i),
    \]
    where $C_j^{\text{mem}}$ and $C_j^{\text{cpu}}$ are the memory and CPU 
    capacities of the worker node $N_j$ on which the scheduler is running. This 
    factor increases when the largest pending task occupies a large fraction of 
    the node’s resources and the function itself is expensive to cold‑start. It 
    encourages the scheduler to keep executing $F_i$’s tasks consecutively, 
    thereby raising the chance of reusing a warm sandbox.

    \item \textbf{Starvation‑avoidance factor} ($\text{factor}_2$).  
    Let $W_{\max}$ be the longest waiting time among all pending tasks of $F_i$,
    and let $\overline{T}(F_i)$ be the average execution duration of $F_i$ tasks. 
    The starvation factor is zero if no task has been waiting excessively; otherwise 
    it grows with the cumulative waiting time and the total work left in the~queue:
    \[
    \text{factor}_2 = 
    \begin{cases}
    0, & \text{if } W_{\max} < 3 \cdot \overline{T}(F_i),\\[4pt]
    \overline{T}(F_i) \cdot |Q_i| + W_{\max}, & \text{otherwise}.
    \end{cases}
    \]
    Here $|Q_i|$ is the total number of tasks in $F_i$’s current queue. This 
    mechanism ensures that a function whose tasks have been waiting too long 
    receives a priority boost, preventing indefinite starvation.

    \item \textbf{Scaling‑efficiency factor} ($\text{factor}_3$).  
    Let $t_{\text{first}}$ and $t_{\text{last}}$ be the tasks with the smallest 
    and largest predicted resource demands in $F_i$ current queue, respectively. 
    Under the assumption that all invocations of the same function share the same 
    CPU‑to‑memory ratio (see \S\ref{subsec:modeling}), the ratio can be computed 
    using either dimension; we use memory for concreteness:
    \[
    \text{factor}_3 = \frac{t_{\text{first}}^{\text{mem}}}{t_{\text{last}}^{\text{mem}}}.
    \]
    A large ratio (close to $1$) indicates that the queue’s resource‑demand range 
    is narrow; consequently, a sandbox that starts with $t_{\text{first}}$ will 
    require little additional scaling to process the remaining tasks. A small 
    ratio (\emph{i.e.}, a wide demand spread) implies that abandoning the current 
    queue would incur less “waste” of scaling effort, which is reflected by the 
    negative weight $\omega_3$.
\end{itemize}

Finally, to further promote sandbox reuse, the priority of any function that already possesses one or more warm sandboxes is \textbf{doubled}. The combined priority is recomputed at every scheduling event, enabling the scheduler to adapt dynamically to changing workload patterns.