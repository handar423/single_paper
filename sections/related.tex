\vspace{-10pt}
\section{Related Work}
\label{sec:related}

\paraf{Serverless runtime resource configuration.}
Optimizing per-instance resource allocation for serverless functions is a well-explored area of research. A prevalent method is profile-guided static tuning~\cite{wang2018peeking, ShahradFGCBCLTR20}, involving benchmarking each function across various resource setups (\emph{e.g.}, memory sizes, CPU cores) to establish a latency-vs-cost curve. The least costly configuration that fulfills the service level objectives (SLOs) is chosen for deployment. Other works~\cite{pu2019shuffling, JinZXZHLJ23, zhang2024jolteon, ZhangTKCS21, ORION_MahgoubYSECB22,AQUATOPE_ZhouZD23} go further to optimize resource allocation for serverless workflows, by modeling the relationship between E2E latency and individual function resources. Cloud providers' tools (\emph{e.g.}, AWS' power tuning tool~\cite{aws_lambda_power_tuning}) employ this technique to assist users in optimizing memory allocations. Unfortunately, all of them assume a static and uniform resource allocation per function, ignoring the significant per-request resource usage variations. Such static allocation can lead to over-provisioning for lightweight requests, resulting in suboptimal utilization. By enabling dynamic resizing of serverless function resources, our work addresses this limitation, allowing more efficient resource~usage.

\parabf{Instance Orchestration and Placement.}
This line of work focuses on managing the lifecycle, quantity, and location of function instances, typically under a static per-instance resource model. A primary concern is optimizing the instance count to balance cost and cold-start latency. Strategies include keeping instances alive in a keep-alive window~\cite{ShahradFGCBCLTR20}, predictive pre-warming based on demand patterns~\cite{ORION_MahgoubYSECB22, RoyPT22}, and dynamic scaling based on real-time metrics~\cite{aws_scaling_window, aws_scaling_eq, knative_scaling, AQUATOPE_ZhouZD23}. To improve placement efficiency, recent work considers several dimensions: i) Cache awareness, which reuses instance states (\emph{e.g.}, through copy-on-write cloning) to reduce startup overhead~\cite{SOCK_OakesYZHHAA18, li2022help}; ii) Cost-optimized placement, which matches functions to specialized or lower-cost hardware (e.g., spot instances, GPUs) to reduce deployment expenses~\cite{RoyPT22, DuLJXZC22}; and iii) Data affinity, which co-locates dependent tasks in serverless workflows to minimize data transfer, as seen in systems like Wukong~\cite{Wukong_CarverZWAWC20}, FaaSFlow~\cite{FaaSFlow22}, ORION~\cite{ORION_MahgoubYSECB22}, and Ditto~\cite{JinZXZHLJ23}. However, a fundamental premise shared by these approaches is the assignment of a static, uniform resource capacity to each function instance. In contrast, \sysname explicitly accounts for resource heterogeneity, aligning complementary workload resource profiles with node capacity through instance placement, thereby minimizing fragmentation and resizing overhead.

\parabf{Request and Task Scheduling.}
For request and task scheduling, prior work has focused on improving scheduling efficiency, load balancing, and resource cost optimization. For instance, Hermod is cost-, load-, and locality-aware to optimize consolidation and reduce cold starts~\cite{SoCC22:Hermod}. XFaaS defers delay-tolerant functions and dispatches calls globally to handle load spikes~\cite{sahraei2023xfaas}. Atoll employs a tiered architecture with semi-global schedulers and sandbox-aware routing~\cite{soccAtoll}. Kaffes et al. advocate for a centralized, core-granular scheduler to eliminate queue imbalance~\cite{soccKaffes}. Golgi minimizes costs through carefully managed overcommitment based on historical usage~\cite{soccGolgi}. However, these scheduling policies are largely ignore the per-request resource demand variations. Our monotonic scale-up scheduling policy reorders function requests to ensure resource demands increase monotonically, thereby amortizing cold start overhead and avoiding over-provisioning.