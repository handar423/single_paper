\section{Background and Motivation}
\label{sec:background}

\subsection{Resource Management of Serverless Computing}
\label{sec:background:resource-management}

% 1. serverless的资源管理模式: 静态资源分配管理
% 把 resource allocation 分为两个步骤，一个是节点-level的资源分配，即运行时的核数和内存，另一个cluster-level的资源分配，即是函数容器副本扩缩容和放置，即决定副本数量、并将容器放置到具体的节点。

Serverless computing has emerged as a promising cloud paradigm, allowing developers to focus on application logic by abstracting away infrastructure tasks such as provisioning, scaling, and maintenance. In this event-driven model, users simply provide code (functions) with basic resource specifications and triggers. The platform then orchestrates the entire lifecycle\textemdash provisioning sandboxed environments, performing on-demand autoscaling, and managing pay-per-use billing. Consequently, efficient resource management within these platforms is essential to ensure high performance, maximize utilization, and minimize costs.

In practice, resource management in serverless platforms encompasses two primary dimensions: i) per-instance resource configuration, which defines the resource limits for individual function instances; and ii) per-function orchestration, which governs autoscaling for instance count and placement for node~selection.

\yunshan{\parabf{Per-instance resource allocation.}
% 1) 静态的运行时资源配置：介绍 lambda 支持指定固定资源规格，及其对延迟和开销的影响，所以选取一个所有请求都不违背qos的配置
Existing serverless computing systems from both industry~\cite{Doc:Azure_Functions_Hosting,
Doc:GCP_CloudRun_Mem, aws_lambda} and academia~\cite{FaaSCache, ShahradFGCBCLTR20, wang2018peeking, pu2019shuffling}
allocate a uniform amount of resources for all instances of a given function. In AWS Lambda, a developer specifies a memory size (\emph{e.g.}, 128 MB--10,240 MB), from which the platform derives proportional vCPU and network bandwidth shares~\cite{aws_lambda}. Developers may also set ephemeral storage per instance (default 512 MB, configurable up to a few gigabytes). These limits remain fixed throughout the instance lifetime. Similar static per-instance specifications are used by Azure Functions~\cite{Doc:Azure_Functions_Hosting} and Google Cloud Run~\cite{Doc:GCP_CloudRun_Mem}. This approach presumes uniform resource demand across function requests. Consequently, configuration optimizations focus on profile-guided parameter tuning~\cite{aws_lambda_power_tuning,FaaSCache, ShahradFGCBCLTR20, zhang2024jolteon, JinZXZHLJ23}: by profiling across discrete configurations, they derive latency–cost curves to select the cheapest configuration that satisfies a given SLO, thereby minimizing cost under QoS constraints.


\parabf{Instance scaling and placement.}
% 2）根据静态资源配置进行容器扩缩容放置：现有的 serverless 系统大多基于静态资源配置进行容器放置调度
% 仅容器启动、离开时比较资源：只需维护节点已分配资源和未分配资源总计，任务到来时可以分配到未分配资源>静态请求资源的节点上，更新已分配资源、未分配资源，函数容器销毁时释放资源、更新已分配资源、未分配资源即可
Existing serverless systems perform scaling~\cite{aws_scaling_window, aws_scaling_eq, knative_scaling} and placement based on a static per-function resource requirement. While autoscalers adjust replica counts in response to load, schedulers employ a simple resource accounting model where each node tracks allocated versus available capacity. The resource counters are updated only during lifecycle events\textemdash decremented upon placement and incremented upon termination\textemdash ignoring actual usage during execution. An instance is admitted to a node only if the available capacity meets the function’s static requirement (\emph{e.g.}, memory, vCPU). For instance scaling and placement, recent research has pursued two main optimizations: (i) for autoscaling, using prediction to right-size replica counts and prewarm functions~\cite{ShahradFGCBCLTR20, ORION_MahgoubYSECB22}; and (ii) for placement, improving efficiency through cache-aware~\cite{SOCK_OakesYZHHAA18}, heterogeneity-aware~\cite{RoyPT22}, and data-affinity strategies~\cite{Wukong_CarverZWAWC20, ORION_MahgoubYSECB22}.

Taken together, the prevailing serverless paradigm relies on modeling each function with a static, uniform resource profile. However, our characterization reveals that this static paradigm conflicts with a dual-layered heterogeneity: diverse resource profiles across functions and variable demands within each function. In the next two subsections, we first quantify these heterogeneities (\S\ref{sec:background:demand-analysis}) and then analyze the system-level challenges they pose to dynamic resource allocation (\S\ref{sec:background:challenges}).}

% 2. serverless 函数资源需求分析
\subsection{Analysis of Resource Requirements of Functions}
\label{sec:background:demand-analysis}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{figures/background/cpu_memory_ratio_histogram.pdf}
  \caption{Heterogeneous CPU:memory ratios across functions.}
  \label{fig:cpu_mem_ratio_hist}
\end{figure}

% 1) 函数级别需求分布
\paraf{Resource Usage Variation Across Functions.}
The resource demands of serverless functions are highly heterogeneous. We quantify this heterogeneity by analyzing the execution traces of 162 real-world functions from the open-source Huawei dataset~\cite{huawei_dataset}, calculating the average CPU-to-memory ratio (core/GB) for each. Figure~\ref{fig:cpu_mem_ratio_hist} presents the distribution of these ratios across all functions.\footnote{The statistical results shown are from [Request Table Module, Region 4] part in dataset~\cite{huawei_dataset}. We observed similar distributions across different parts of the dataset.} The distribution is highly dispersed, with ratios ranging from 0.09 to 9.20 and a high CV of 62.43\%, indicating that some functions are intensely compute-intensive while others are predominantly memory-intensive. This substantial variation in intrinsic resource profiles implies that a scheduler unaware of such differences is prone to creating resource fragmentation, where one resource type is exhausted on a node while another remains underutilized.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/background/cdf_cpu_cv_regions.pdf}
    \caption{Resource usage variations of serverless function.}
    \vspace{-15pt}
\label{fig:usage_variation}
\end{figure}

% 2) 调用级别资源占用
\paraf{Resource Usage Variation Across Requests.}
Resource demand also varies significantly across different requests of the same function. We compute the coefficient of variation (CV) for the CPU usage of each function across its request trace. The distribution of these CV values is presented in Figure~\ref{fig:usage_variation}. Our analysis shows that variability is the norm rather than the exception: for 54\% of functions, the CV exceeds 50\%, and an additional 26\% have a CV between 15\% and 50\%. This widespread and often substantial variability directly contradicts the assumption of uniform demand underlying static allocation. Consequently, any static resource allocation must provision for the peak demand of such variable functions, inevitably leading to low average utilization.

% 3) Opportunity: 函数动态运行时资源调整 
% 现有容器框架支持动态运行时资源分配（具体机制或者接口），以及其好处（节省资源减少浪费）
\parabf{Opportunity: Dynamic in‑place runtime resource adjustment}
A key opportunity to address the intra-function variability lies in recent container 
runtime enhancements that support dynamic, in-place resource adjustment. Specifically, 
platforms like Kubernetes now allow the resource allocations (CPU and memory) of 
a running pod to be updated without restarting it, via mechanisms such as the Pod 
resize subresource and live cgroup updates~\cite{Doc:K8s_Pod_Resize, Doc:K8s_CRI_Update}.
This capability enables a container's assigned resources to be aligned in real 
time with the actual demand of the incoming request. It thus provides a 
foundational mechanism to move beyond static per-function allocation, toward a 
fine-grained model where resources can be expanded or contracted per task. 
It absorbs demand bursts, reduces stranded resources from over-provisioning, 
and lowers the risk of SLO violation\textemdash while preserving performance 
benefits of a warm instance.

% 3. 动态资源分配的挑战

\subsection{Challenges in Dynamic Resource Allocation for Functions}
\label{sec:background:challenges}

% 1）挑战1：副本放置挑战：动态资源分配打破了原有的静态资源分配假设，容器的资源需求不再是固定的，不能仅在容器启动和销毁时考虑节点资源分配
% strawman solution：发生扩容剩余节点资源不足时直接evict其他同节点容器，但带来缓存损失，且可能放大（相对较大规格的容器扩容，可能一下需要evict多个小规格容器）
% appeal for Minimal-Disruption Placement Scaling with multi-resource balance awareness

\paraf{Challenge 1: Placement under dynamic in-place resizing.} Dynamic in-place resizing disrupts the fixed-size bin-packing model: a sandbox’s footprint may expand while active, making node feasibility assessments at start/stop times inadequate. A simplistic approach is to evict co-located instances to make space when upscaling faces insufficient capacity. However, this naive strategy risks evicting other co-located function instances with increased resource demand. This issue is exacerbated when a large upsizing occurs under constrained resource availability, resulting in eviction amplification: a single significant resize event can force the eviction of multiple smaller co-located instances, thereby hurting cache locality and increasing tail latency. Additionally, fragmentation can occur when resource demand descreases. These problems may lead to low resource utilization and application throughput.

\yunshan{Therefore, an intelligent placement strategy must anticipate and mitigate the disruptive effects of resizing. Its goal is twofold: i) to balance CPU and memory usage per node from the outset, preventing resource fragmentation and minimizing the need for disruptive migrations; and ii) when migrations are unavoidable, to execute them in a manner that minimizes overall instance disruption. By promoting managed, minimal churn, such a strategy safeguards warm states (maximizing reuse) and maintains high packing efficiency and throughput even during dynamic resizing.}


\paraf{Challenge 2: High overhead of downsizing.} Dynamic in-place scaling is asymmetric: upsizing is swift, but downsizing incurs high costs. Increasing the memory allocation takes effect immediately, whereas reducing it triggers cgroup reclamation, page-cache eviction, and compaction, potentially stalling both runtime and kernel. \yunshan{Since resizing affects the request path, this slow reclaim process increases tail latency and queueing, which in turn lowers overall node throughput even when average capacity suffices.}

Two baseline strategies struggle to handle bursty, diverse functions effectively. The first approach is to over-provision numerous replicas, each fixed to a narrow size band, to avoid the need for resizing. However, this leads to significant resource fragmentation and poor utilization due to excessive headroom. The second approach is to maintain fewer replicas and resize them dynamically for every request. Unfortunately, such frequent adjustments incur high memory reclamation overheads, disrupt page caches and JIT states, and increase scheduler interference. Ultimately, both strategies degrade system throughput, negating the theoretical advantages of dynamic scaling. \yunshan{These inefficiencies are illustrated in Figure~\ref{fig:vertical_scaling_scheduling}, which contrasts the wasted headroom of static over-provisioning with the reclamation overhead of naive dynamic resizing.}

\yunshan{Therefore, a practical dynamic scaling scheduler must achieve the following goal: it should adapt resource allocation to variable request-level demand without incurring frequent downsizing overheads. This necessitates a mechanism that can batch or reorder requests to create resource usage patterns that are conducive to monotonic (upward-only) adjustments within short time windows, thereby amortizing reclamation costs and preserving warm instance state.}

\begin{figure}[t]
  \centering
  \subfloat[]{\includegraphics[width=0.47\linewidth]{figures/background/ex_1.pdf}}
  \hfil
  \subfloat[]{\includegraphics[width=0.47\linewidth]{figures/background/ex_2.pdf}}
  \caption{Inefficient scaling strategies under variable workloads. (a) Static over-provisioning. (b) Naive dynamic resizing.}
  \label{fig:vertical_scaling_scheduling}
\end{figure}

% # 本文优化目标（场景：高负载、高并发，核心目标：资源利用率和任务吞吐量）