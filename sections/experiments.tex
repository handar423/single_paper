\vspace{-5pt}
\section{Evaluation}
\label{sec:experiments}
\vspace{-3pt}

In this section, we first evaluate the overall performance of \sysname against 
state-of-the-art systems—Palette~\cite{abdi2023palette} and 
FaaSCache~\cite{FaaSCache}—in \S\ref{sec:experiments_comparsion}. Next, 
we conduct an in-depth analysis of the effectiveness of the key techniques 
employed in \sysname in \S\ref{sec:experiments_technique}. Finally, we investigate 
the scalability of \sysname in large-scale cluster environments in \S\ref{sec:experiments_overhead}.

\begin{table*}[t]
    \vspace{-10pt}
    \caption{Benchmark characteristics: resource profiles and execution times.\label{tab:setup}}
    \centering
    \begin{tabular}{r c c c c c c}
        \toprule
        \textbf{Workload} & \textbf{Type} & \makecell[c]{\textbf{CPU} \\ \textbf{(cores)}} & \makecell[c]{\textbf{Duration} \\ \textbf{(s)}} & \makecell[c]{\textbf{Mem} \\ \textbf{(GB)}} & \makecell[c]{\textbf{CPU/Mem} \\ \textbf{(core/GB)}} & \textbf{Description} \\
        \midrule
        graph-bfs      & Scientific     & 1.5 & 1.0 & 3.0 & 1.50 & Breadth-first search traversal implemented using igraph. \\
        graph-mst      & Scientific     & 1.5 & 1.0 & 4.2 & 1.50 & Computing minimum spanning tree using igraph. \\
        graph-pagerank & Scientific     & 4.0 & 4.0 & 21.5 & 1.00 & PageRank algorithm execution with igraph. \\
        dna-visualisation & Scientific & 1.0 & 2.0 & 7.6 & 0.50 & Generating visualization data from DNA sequences. \\
        dynamic-html   & Webapps        & 1.0 & 1.0 & 0.15 & 1.00 & Rendering dynamic HTML content from a template. \\
        uploader       & Webapps        & 2.0 & 2.0 & 2.5 & 1.00 & Uploading files from a given URL to cloud storage. \\
        compression    & Utilities      & 1.0 & 2.0 & 1.5 & 0.50 & Compressing multiple files into a ZIP archive for download. \\
        image-recognition & ML          & 2.5 & 4.0 & 1.6 & 0.63 & Performing image classification using ResNet and PyTorch. \\
        thumbnailer    & Multimedia     & 0.5 & 2.0 & 5.0 & 0.25 & Creating thumbnail images from input pictures. \\
        video-processing & Multimedia   & 1.5 & 4.0 & 11.0 & 0.38 & Adding watermarks and generating GIFs from video files. \\
        \bottomrule
        \vspace{-15pt}
    \end{tabular}
\end{table*}

\subsection{Evaluation Setup}
\label{sec:experiments_setup}

\parabf{Benchmarks.}
We selected ten benchmark applications from a recent open-source serverless 
benchmarking suite~\cite{copik2021sebs, benchmark_10} to evaluate the performance 
of \sysname across diverse workloads. Table~\ref{tab:setup} summarizes the basic 
distribution and resource profiles of these benchmarks. The selected benchmarks 
span multiple categories—including Web Apps, Multimedia, Utilities, Machine Learning, 
and Scientific Computing—and exhibit substantial variation in resource requirements. 
As shown in the table, their memory consumption ranges from 0.5--4 GB, CPU usage 
from 0.5--4 cores, and execution time from 0.15--21.5 seconds. Crucially, the 
CPU-to-memory ratio (shown in the table) varies widely from 0.25 to 1.50 cores/GB, 
highlighting the spectrum from memory-intensive (\emph{e.g.}, thumbnailer) to 
CPU-intensive (\emph{e.g.}, graph-bfs) workloads.

\parabf{Workload generation.} We generate user traffic based on the 
Azure Functions Dataset~\cite{ShahradFGCBCLTR20}. For each benchmark, we randomly 
sample a function invocation trace from the top 20\% most frequently invoked 
functions, which account for the majority of resource consumption in production 
environments~\cite{ShahradFGCBCLTR20}, representing the critical bottleneck for 
cluster performance optimization. Each trace's invocation rate is proportionally 
scaled so that the aggregated resource demand approaches the total cluster capacity, 
and the invocation frequencies across the ten benchmarks are adjusted to be nearly 
identical, with relative differences under 2\%. Given that the original trace 
data is recorded at minute-level granularity, intra-minute request arrivals are 
modeled as a Poisson process, a standard approach for independent random events 
in systems and performance analysis~\cite{Gallager2013Stochastic}. Following common 
industrial practice~\cite{aws_scaling_window, knative_scaling_window}, the scaling 
interval is set to one minute.

To accurately model real-world serverless functions, we first analyze 
the resource usage distribution in the Azure Functions dataset~\cite{ShahradFGCBCLTR20}. 
Using both the Shapiro-Wilk~\cite{SHAPIRO1965} and Kolmogorov-Smirnov~\cite{f4feef4d-12ae-314a-a10b-d1f2768e819a} 
tests for normality, we find that over 95\% of functions exhibit memory usage 
patterns that do not significantly deviate from a normal distribution (p-value $>$ 0.05). 
Based on this empirical observation, we model the per-function resource demand 
as normally distributed across requests. For our evaluation, we randomly select 
ten functions from the dataset and perform Gaussian fitting to estimate the 
variance of their memory usage. We then adjust the input data size and parameters 
of each benchmark so that the actual resource requirements reflect the sampled 
variance. Since the dataset primarily captures memory demand, CPU allocation is 
assumed to scale proportionally with the fixed CPU-to-memory ratio for each benchmark.

\parabf{Cluster configuration.} Experiments are conducted on CloudLab~\cite{cloudlab} 
using a cluster of nine c6620 instances (56 cores total, 2$\times$ Intel Xeon 
Gold 5512U @ 2.1GHz, 128GB memory). Eight nodes serve as worker servers and one 
as the scheduler and request generator. To emulate the heterogeneous resource 
availability typical of production clusters, each worker node is allocated only 
a subset of its total resources. Unless otherwise specified, the available resource 
proportions across nodes are sampled from a normal distribution with a standard 
deviation~of~0.85.

\parabf{Baselines.} We evaluate \sysname against two state-of-the-art resource 
management approaches for serverless platforms, both of which optimize cluster 
efficiency from complementary perspectives. For Palette, each benchmark is assigned 
a unique color, and functions with the same color are co-located on the same server 
to enhance spatial locality. Inspired by caching systems, FaaSCache employs a 
Greedy-Dual keep-alive strategy, which assigns a priority to each kept-alive 
instance and evicts low-priority instances first under resource pressure. To 
ensure a fair comparison and minimize the impact of engineering artifacts, we 
reimplement both baselines in our prototype environment. Furthermore, while 
maintaining controlled conditions, we enable Kubernetes pod in-place scaling 
across all systems during evaluation (see prototype implementation details in 
Section~\ref{sec:implementation}).

\parabf{Metric.} We evaluate system performance using CPU/memory utilization 
and task throughput as primary metrics. To verify that the scheduling algorithm 
in \sysname does not lead to task starvation, we also compare the 95th percentile 
tail latency of task processing. Each data point is collected from a one-hour 
test run, with the performance data from the first five minutes excluded to 
eliminate the impact of initial cold starts.

\subsection{Benefits of \sysname}
\label{sec:experiments_comparsion}

\begin{figure*}[t]
    \centering
    \subfloat[]{\includegraphics[width=0.32\linewidth]{figures/Eval/distribution_CPU Utilization.pdf}%
    \label{fig:eval_dis_cpu}}
    \hfil
    \subfloat[]{\includegraphics[width=0.32\linewidth]{figures/Eval/16worker_distribution_Memory Utilization.pdf}%
    \label{fig:eval_dis_memory}}
    \hfil
    \subfloat[]{\includegraphics[width=0.32\linewidth]{figures/Eval/distribution_Task throughput.pdf}%
    \label{fig:eval_dis_throughput}}
    \caption{Performance comparison under different cluster resource distributions. (a) CPU Utilization. (b) Memory Utilization. (c) Task throughput.}
    \vspace{-15pt}
\label{fig:eval_dis}
\end{figure*}

To better demonstrate the performance of \sysname under different deployment 
scenarios, we first compare the overall performance between \sysname and baselines 
under different cluster resource distributions, benchmark numbers and workload 
distributions.

\parabf{Comparison under different cluster resource distributions.} Figure~\ref{fig:eval_dis} 
compares the performance of \sysname and baselines under different cluster resource 
distributions. We introduce two resource availability patterns: uniform 
and Zipf distributions. Under the uniform distribution, each server is configured 
with 75\% of its total resources available. In the Zipf-based distribution, the 
fraction of available resources per server follows a Zipf distribution parameterized 
with $\alpha = 0.8$ across $n = 8$ resource tiers. 

Across all resource distribution patterns, \sysname consistently improves both 
CPU and memory utilization, achieving relative improvements of 43.61\%--55.53\%
and 41.41\%--67.01\% over the baselines, respectively. Furthermore, \sysname 
demonstrates performance gains in task throughput, with improvements ranging from 
51.52\% to 103.71\%.

To address the potential concern of task starvation, we examine the impact on 
end-to-end performance. As shown in Table~\ref{tab:latency}, \sysname reduces 
the overall 95th percentile latency by 30.63\%--42.42\% compared to the baselines. 
This result indicates that the efficiency gains of \sysname are not achieved by 
systematically delaying particular functions. Instead, the improvement 
in resource utilization and throughput is achieved alongside enhanced and more 
predictable end-to-end performance, even when co-locating diverse benchmarks with 
varying resource demands and execution characteristics.

The performance gains of \sysname are primarily attributed to two mechanisms. 
First, the complementary swarming request routing strategy enables individual 
nodes to serve fewer functions, thereby increasing instance reuse probability. 
This approach also aligns node workload with resource capacity, avoiding resource 
waste caused by load-resource mismatch. Second, the monotonic scale-up 
scheduling policy reduces both resource over-provisioning and cold-start frequency, 
further boosting overall resource utilization.

\begin{table}[t]
    \vspace{-10pt}
    \caption{Performance comparison under multiple workflows.\label{tab:latency}}
    \begin{tabularx}{0.47\textwidth}{ 
        >{\raggedright\arraybackslash}X 
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X }
        \toprule
        \multirow{2}{*}{\makecell[l]{\textbf{Resource} \\ \textbf{Distribution}}} & \multicolumn{3}{c}{\textbf{95th Tail Latency (s)}} \\ \cline{2-4}
         & \raisebox{-1pt}{\textbf{Palette}} & \raisebox{-1pt}{\textbf{FaaSCache}} & \raisebox{-1pt}{\textbf{\sysname}} \\
        \hline 
        Norm-85 & 1071 & 1104 & \textbf{743} \\
        Uniform-75\%& 1054 & 1063 & \textbf{729} \\
        Zipf-80 & 1372 & 1335 & \textbf{790} \\
        \bottomrule
    \end{tabularx}
\end{table}

\begin{figure*}[t]
    \centering
    \subfloat[]{\includegraphics[width=0.32\linewidth]{figures/Eval/func_number_CPU Utilization.pdf}%
    \label{fig:eval_function_cpu}}
    \hfil
    \subfloat[]{\includegraphics[width=0.32\linewidth]{figures/Eval/func_number_Memory Utilization.pdf}%
    \label{fig:eval_function_memory}}
    \hfil
    \subfloat[]{\includegraphics[width=0.32\linewidth]{figures/Eval/func_number_Task throughput.pdf}%
    \label{fig:eval_function_throughput}}
    \caption{Performance comparison under different function numbers. (a) CPU Utilization. (b) Memory Utilization. (c) Task throughput.}
    \vspace{-15pt}
\label{fig:eval_function}
\end{figure*}

\parabf{Comparison under different benchmark numbers.}
Next, we evaluate \sysname and the baselines under different workload scales. 
Beyond the default configuration of 10 benchmarks, we provide two additional scenarios:

\begin{itemize}
    \item \textbf{5 benchmarks}: A randomly selected subset comprising \textbf{video-processing}, 
    \textbf{graph-pagerank}, \textbf{image-recognition}, \textbf{dna-visualization}, 
    and \textbf{thumbnailer}. To maintain comparable total load, the 
    invocation rate for each function in this set is doubled;

    \item \textbf{20 benchmarks}: The original 10 benchmarks plus their replicas. Each 
    replica has the same code, resource profile, and runtime as the original but 
    is considered a distinct function, preventing instance reuse between the 
    original-replica pairs. The invocation rate per function is halved to 
    maintain the aggregate user traffic.
\end{itemize}

As shown in Figure~\ref{fig:eval_function}, the performance advantage of \sysname 
over the baselines grows as the number of benchmarks increases from 5 to 20. 
Specifically, the improvement in CPU utilization rises from 17.40\%--16.90\% 
to 44.89\%--44.06\%, memory utilization gains increase from 41.81\%--43.26\% 
to 55.42\%--57.37\%, and task throughput improvements expand from 45.22\%--51.06\% 
to 75.45\%--78.98\%.

This scaling trend can be attributed to the complementary swarming routing. With 
fewer benchmarks, each node already operates on a limited set of functions, 
leaving limited room for further optimization through instance reuse. In contrast, 
when the workload is larger and more diverse, the routing reduces the number of 
distinct functions each node handles. This effect elevates the instance reuse 
rate, which in turn improves overall~efficiency.

\begin{figure*}[t]
    \centering
    \subfloat[]{\includegraphics[width=0.32\linewidth]{figures/Eval/skew_CPU Utilization.pdf}%
    \label{fig:eval_workload_cpu}}
    \hfil
    \subfloat[]{\includegraphics[width=0.32\linewidth]{figures/Eval/skew_Memory Utilization.pdf}%
    \label{fig:eval_workload_memory}}
    \hfil
    \subfloat[]{\includegraphics[width=0.32\linewidth]{figures/Eval/skew_Task throughput.pdf}%
    \label{fig:eval_workload_throughput}}
    \caption{Performance comparison under different workload distributions. (a) CPU Utilization. (b) Memory Utilization. (c) Task throughput.}
    \vspace{-15pt}
\label{fig:eval_workload}
\end{figure*}

\parabf{Comparison under different workload distributions.} 
Finally, we examine the impact of workload distribution on scheduling performance. 
In addition to the default resource distribution (see~\ref{sec:experiments_setup}), 
we introduce a Zipf-based load pattern. Specifically, we adjust the $\alpha$ parameter 
of a Zipf distribution (with n = 10) to obtain a probability set that approximates 
the Pareto principle (\emph{i.e.}, the 80/20 rule), where the sum of the two largest 
probabilities is about 0.8. We then scale the invocation rate of each benchmark 
according to this distribution, preserving the total aggregate invocation frequency 
while ensuring that the two most frequently invoked functions—randomly chosen as 
\textbf{thumbnailer} and \textbf{video-processing}—collectively account for 80\% 
of all invocations.

It can be observed from Figure~\ref{fig:eval_workload} that, compared to the default 
uniform distribution, the optimization margin of \sysname under the Zipf workload 
distribution is relatively smaller. Under the Zipf distribution, \sysname achieves 
an increase in CPU utilization of 23.20\%--23.58\%, memory utilization of 
27.04\%--28.50\%, and task throughput of 29.20\%--31.34\% over the~baselines.

\looseness=-1
This result stems from our experimental configuration: the Zipf distribution 
adhering to the 80/20 rule implies that only two functions bear the majority of 
the workload, which naturally grants them more instance reuse opportunities. In 
contrast, while the real-world workload distribution also exhibits skew, it 
involves a larger number of high-load functions competing for node resources 
simultaneously. This competition creates more optimization potential for 
\sysname. Moreover, even with only two high-load functions, \sysname 
still achieves around 20\%--30\% higher resource utilization and task throughput 
than the baselines. This gain is primarily because the monotonic scale-up 
scheduling policy reduces both instance resource over-provisioning and 
cold-start frequency through request reordering, thereby enhancing overall 
resource~efficiency.

\begin{figure}[t]
    \centering
    \subfloat[]{\includegraphics[width=0.9\linewidth]{figures/Eval/detail_cpu.pdf}%
    \label{fig:eval_detail_cpu}}
    \hfil
    \subfloat[]{\includegraphics[width=0.9\linewidth]{figures/Eval/detail_memory.pdf}%
    \label{fig:eval_detail_memory}}
    \caption{Detail Resource Comsupmtion of \sysname and baselines. (a) CPU Detail Comsupmtion. (b) Memory Detail Comsupmtion.}
    \vspace{-10pt}
\label{fig:eval_detail}
\end{figure}

\parabf{Performance analysis.} To further demonstrate the reasons for \sysname 
performance advantage, Figure~\ref{fig:eval_detail} provides a detailed 
illustration of the average resource consumption of \sysname and the baselines 
during task processing. We categorize CPU and memory resources into the following 
four~types: 

\begin{itemize}
    \item \textbf{Task Execution}: resources allocated for processing tasks;
    \item \textbf{Cold Start}: resources consumed during the initialization of instances;
    \item \textbf{Standby Instance}: resources held by ready but currently idle instances;
    \item \textbf{Idle Resource}: available but unallocated resources.
\end{itemize}

Taking CPU utilization as an example, compared to the baselines, \sysname increases 
the resource share for Task Execution by 45.47\%--43.61\%, for Standby Instance 
by 110.87\%--125.58\%, and for Idle Resource by 43.59\%--47.37\%, while 
it reduces the share for Cold Start by 83.65\%--83.56\%. A similar trend is 
observed for memory utilization.

The resource utilization advantage of \sysname stems from the reduction in cold 
starts. This is achieved through two strategies. First, the complementary swarming 
routing concentrates a narrower set of functions on each node, which 
increases instance reuse opportunities. Second, the monotonic scale-up 
scheduling policy reorders requests and dynamically adjusts instance resources, 
enabling instances to serve varying demands throughout their lifecycle without 
repeated cold starts or static over-provisioning.

The resources conserved from fewer cold starts are reallocated in two ways: they 
increase the share for task execution while expanding the pool of standby instances. 
Consequently, a larger ready-to-use instance pool is maintained, raising the 
probability of warm starts.

\subsection{Effectiveness of \sysname}
\label{sec:experiments_technique}

In this section, we investigate the impact of the techniques employed by 
\sysname. We evaluate \sysname using the following two simplified versions:
\begin{itemize}
    \item \textbf{Routing*}: This variant uses only the complementary swarming 
    request routing strategy, while tasks within each node are scheduled using 
    a simple FIFO policy.
    \item \textbf{Scheduling*}: This variant uses only the monotonic scale-up 
    scheduling policy within each node, while user requests are randomly 
    assigned across worker nodes.
\end{itemize}

{
\begin{figure}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{figures/Eval/workflow_xiaorong.pdf}
    \caption{Performance comparison between \sysname, baselines and simplified versions.}
    \label{fig:technique}
    \vspace{-15pt}
\end{figure}
}

Figure~\ref{fig:technique} shows the performance comparison between \sysname and 
simplified versions. We also provide the evaluation results of the baselines for 
reference.

As shown, the performance of the Routing* version lies between the baselines 
and \sysname. Compared to the baselines, it achieves improvements of 20.07\%--21.63\%, 
16.90\%--17.23\%, and 17.31\%--15.74\% in CPU utilization, memory utilization, 
and task throughput, respectively. However, it still falls short of \sysname by 
16.39\%, 17.33\%, and 23.62\% in the metrics. These results demonstrate that the 
complementary swarming routing strategy alone can effectively enhance 
scheduling performance through load partitioning. Furthermore, it requires integration 
with the monotonic scale-up scheduling policy for fine-grained, 
node-internal resource management to achieve optimal performance.

The performance of the Scheduling* variant reveals a more nuanced trade-off. It 
underperforms \sysname in resource utilization, with CPU and memory utilization 
lower by 13.35\% and 29.99\%, respectively. Despite this, it achieves a 27.95\% 
higher task throughput. This discrepancy occurs because Scheduling* prioritizes 
tasks with lower resource demands, shorter execution times, and higher  
frequencies, without explicitly optimizing for overall server utilization.

When integrated with the complementary swarming routing strategy 
(as in \sysname), each node handles a limited, resource-complementary set of 
functions. This increases the likelihood that tasks with high resource demands 
are selected for instance setup and execution, raising the execution priority for 
subsequent tasks from the same function (see Section~\ref{subsec:priority-design}). In 
contrast, without effective load partitioning, a wider variety of instances compete 
for node resources. In this scenario, resource-intensive instances are served 
mainly through starvation-avoidance mechanisms, receiving only limited execution 
opportunities. The resulting contention leads to frequent instance eviction and 
cold starts, which hinders effective improvement in resource utilization.

\subsection{Scalability}
\label{sec:experiments_overhead}

Finally, we evaluate the scalability of \sysname. Due to testbed limitations, 
this section includes both real and simulated experiments. We first demonstrate 
the performance of \sysname on a larger-scale cluster. Then we
show the system overhead of the \sysname scheduler on up to 1024 nodes.

\begin{figure*}[t]
    \centering
    \subfloat[]{\includegraphics[width=0.32\linewidth]{figures/Eval/16worker_distribution_CPU Utilization.pdf}
    \label{fig:eval_big_cpu}}
    \hfil
    \subfloat[]{\includegraphics[width=0.32\linewidth]{figures/Eval/16worker_distribution_Memory Utilization.pdf}
    \label{fig:eval_big_memory}}
    \hfil
    \subfloat[]{\includegraphics[width=0.32\linewidth]{figures/Eval/16worker_distribution_Task throughput.pdf}
    \label{fig:eval_big_throughput}}
    \caption{Performance comparison in 16-worker cluster. (a) CPU Utilization. (b) Memory Utilization. (c) Task throughput.}
    \vspace{-15pt}
\label{fig:eval_big}
\end{figure*}

\parabf{Larger-scale cluster experiment.}
In the larger-scale cluster experiment, we increased the number of worker nodes 
to 16 and proportionally scaled the invocation frequency, while keeping 
other conditions constant. Figure~\ref{fig:eval_big} shows the performance comparison 
between \sysname and the baselines. Compared to the baselines, \sysname improves 
CPU utilization by 31.80\%--61.75\%, memory utilization by 41.95\%--53.82\%, 
and task throughput by 40.24\%--110.75\%. These results align with those observed 
in the smaller cluster, demonstrating that \sysname maintains its performance 
advantage consistently at scale.

{
\begin{figure}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{figures/Eval/overhead_placement.pdf}
    \caption{The average duration for \sysname to generate a routing plan.}
    \label{fig:overhead_placement}
    \vspace{-10pt}
\end{figure}
}

\parabf{System overhead.}
We next evaluate the overhead of the \sysname scheduler under varying cluster 
sizes and invocation frequencies. To do so, we simulate scheduler load for clusters 
ranging from 64 to 1024 worker nodes. To avoid interference from bursty traffic 
patterns, we generate invocations at a steady rate instead of using the trace 
from the Azure Functions Dataset~\cite{ShahradFGCBCLTR20}. The experimental results 
are presented in Figures~\ref{fig:overhead_placement} and~\ref{fig:overhead_scheduling}. 

Figure~\ref{fig:overhead_placement} shows the average duration for \sysname to generate 
cluster-wide request routing plans under different cluster sizes. It can be observed 
that the generation duration increases gradually with cluster scale but 
remains largely unaffected by invocation frequency. This is because \sysname scales 
the overall load proportionally to the cluster size before computing the routing 
plan. Even with 1024 worker nodes and an invocation rate of $250 \times 10^3$ 
requests per minute, the latency stays below 110 ms. This result indicates that 
the scheduler can generate routing plans promptly, even in large-scale 
clusters, demonstrating the practical scalability of~\sysname.

{
\begin{figure}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{figures/Eval/overhead_scheduling.pdf}
    \caption{The average scheduling overhead of \sysname.}
    \label{fig:overhead_scheduling}
    \vspace{-10pt}
\end{figure}
}

Figure~\ref{fig:overhead_scheduling} shows the scheduling overhead for individual 
tasks under different invocation frequencies. It should be noted that 
routing plan generation is performed by a separate background process within 
the scheduler. The main scheduling process only receives the resulting routing 
plan and assigns tasks accordingly. Therefore, only the scheduling overhead 
contributes to the additional latency introduced by \sysname in the end-to-end 
task execution path. As shown, even under a heavy load of 1024 worker nodes, the 
extra scheduling delay added by \sysname remains below 10 ms. This overhead is 
negligible for the vast majority of FaaS scenarios, confirming that the scheduling 
logic of \sysname is lightweight enough for production-scale deployment.
